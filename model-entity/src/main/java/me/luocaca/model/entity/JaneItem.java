package me.luocaca.model.entity;

import java.util.List;

public class JaneItem{


    /**
     * q : 爬虫
     * page : 9
     * type : note
     * total_count : 28037
     * per_page : 10
     * total_pages : 100
     * order_by : default
     * entries : [{"id":28702113,"title":"【Python实战】用Scrapyd把Scrapy<em class='search-result-highlight'>爬虫<\/em>一步一步部署到腾讯云上，有彩蛋","slug":"af98a1b72b3e","content":"\u2026\u2026接着之前的几篇文章说。我把<em class='search-result-highlight'>爬虫<\/em>已经写好了，而且在本地可以运行了。这个不是最终的目的啊。我们是要在服务器上运行<em class='search-result-highlight'>爬虫<\/em>。利用周末，同时腾讯送的7天云服务器体验也快到期了就在这里再来一篇手把手的将<em class='search-result-highlight'>爬虫<\/em>部署到\u2026\u2026服务器上吧。绝对从0教学。一步一步的来，还有截图让你从『倔强青铜』杀到『最强王者』 为啥要写这篇文章，就是为了让你上『最强王者』！ Scrapy的文章，好多好多，但是99%的文章都是，写完<em class='search-result-highlight'>爬虫<\/em>就完事儿\u2026\u2026了，至于后来怎么用？去哪里用？都没有交带。我这里就交代一种，可以把你的小虫子部署到服务器上！但是怎么部署，当你去百度查『scrapy<em class='search-result-highlight'>爬虫<\/em>部署』的时候，有几篇文章说，用Scrapyd，但是，他们都只是\u2026\u2026","user":{"id":4417378,"nickname":"皮克啪的铲屎官","slug":"c2aa1d94244a","avatar_url":"http://upload.jianshu.io/users/upload_avatars/4417378/3160dc89-7fd3-457e-aa96-1616f9a22080"},"notebook":{"id":25265876,"name":"Python"},"commentable":true,"public_comments_count":1,"likes_count":13,"views_count":1055,"total_rewards_count":0,"first_shared_at":"2018-05-28T04:59:26.000Z"},{"id":6103609,"title":"python异步<em class='search-result-highlight'>爬虫<\/em>","slug":"01431e72b361","content":"\u2026\u2026本文英文原文来自于 500 lines or less -- A Web Crawler With asyncio Coroutines中的对于<em class='search-result-highlight'>爬虫<\/em>的代码的解释 python之父和另外一位\u2026\u2026python大牛实现了简单的异步<em class='search-result-highlight'>爬虫<\/em>来展示python的推荐异步方式和效果。利用协程充分提升<em class='search-result-highlight'>爬虫<\/em>性能。同时也展示了一个简单的异步<em class='search-result-highlight'>爬虫<\/em>编写流程和结构。 将代码下载解压后直接pip install aiohttp\u2026\u2026安装依赖库然后使用命令python3 crawl.py -q xkcd.com就可以直接先体验下异步<em class='search-result-highlight'>爬虫<\/em>的效果 可以先直接看第二部分 使用标准库asyncio和aiohttp 再回头看第一部分 基于生成\u2026\u2026","user":{"id":2422746,"nickname":"treelake","slug":"66f24f2c0f36","avatar_url":"http://upload.jianshu.io/users/upload_avatars/2422746/a597dc58020c.jpg"},"notebook":{"id":7231458,"name":"Python"},"commentable":true,"public_comments_count":0,"likes_count":22,"views_count":3717,"total_rewards_count":0,"first_shared_at":"2016-10-08T15:07:39.000Z"},{"id":42362148,"title":"32个Python<em class='search-result-highlight'>爬虫<\/em>实战项目，满足你的项目慌","slug":"f1d39609db7b","content":"\u2026\u2026<em class='search-result-highlight'>爬虫<\/em>项目名称及简介 一些项目名称涉及企业名词，小编用拼写代替 1、【WechatSogou】- weixin公众号<em class='search-result-highlight'>爬虫<\/em>。基于weixin公众号<em class='search-result-highlight'>爬虫<\/em>接口，可以扩展成其他搜索引擎的<em class='search-result-highlight'>爬虫<\/em>，返回结果是列表\u2026\u2026，每一项是公众号具体信息字典。 2、【DouBanSpider】- douban读书<em class='search-result-highlight'>爬虫<\/em>。可以爬下豆瓣读书所有图书，按评分排名依次存储，存储到Excel中，比如筛选评价人数>1000的高分段书籍；可依据\u2026\u2026不同的类别存储到Excel不同的分类 ，采用User Agent伪装为浏览器进行爬取，并加入随机延时来更好的模仿浏览器行为，避免<em class='search-result-highlight'>爬虫<\/em>被封。 3、【zhihu_spider】- zhihu<em class='search-result-highlight'>爬虫<\/em>。爬取\u2026\u2026","user":{"id":3109680,"nickname":"性感的猫咪","slug":"f52597615465","avatar_url":"http://upload.jianshu.io/users/upload_avatars/3109680/77c0e76efde3"},"notebook":{"id":6436644,"name":"日记本"},"commentable":true,"public_comments_count":0,"likes_count":1,"views_count":30,"total_rewards_count":0,"first_shared_at":"2019-03-04T01:42:14.000Z"},{"id":41231999,"title":"（了解）通用<em class='search-result-highlight'>爬虫<\/em>和聚焦<em class='search-result-highlight'>爬虫<\/em>--<em class='search-result-highlight'>爬虫<\/em>基础教程（python）（二）","slug":"9666214636f2","content":"\u2026\u2026通用<em class='search-result-highlight'>爬虫<\/em>和聚焦<em class='search-result-highlight'>爬虫<\/em> 根据使用场景，网络<em class='search-result-highlight'>爬虫<\/em>可分为 通用<em class='search-result-highlight'>爬虫<\/em> 和 聚焦<em class='search-result-highlight'>爬虫<\/em> 两种.我们主要写通用<em class='search-result-highlight'>爬虫<\/em>。 通用<em class='search-result-highlight'>爬虫<\/em> 通用网络<em class='search-result-highlight'>爬虫<\/em> 是 捜索引擎抓取系统（Baidu、Google、Yahoo等）的重要组成\u2026\u2026部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。 通用搜索引擎（Search Engine）工作原理 通用网络<em class='search-result-highlight'>爬虫<\/em> 从互联网中搜集网页，采集信息，这些网页信息用于为搜索\u2026\u2026引擎建立索引从而提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。 第一步：抓取网页 搜索引擎网络<em class='search-result-highlight'>爬虫<\/em>的基本工作流程如下： 首先选取一部分的种子URL，将\u2026\u2026","user":{"id":4843216,"nickname":"白夜python","slug":"3ba062e4ae6f","avatar_url":"http://upload.jianshu.io/users/upload_avatars/4843216/abb9eb58bbca.jpg"},"notebook":{"id":10051215,"name":"随笔"},"commentable":true,"public_comments_count":0,"likes_count":0,"views_count":13,"total_rewards_count":0,"first_shared_at":"2019-02-12T05:06:15.000Z"},{"id":40662014,"title":"想把python<em class='search-result-highlight'>爬虫<\/em>了解透彻吗？一起盘它 !  !","slug":"dcfbc5a183fb","content":"\u2026\u2026原理 传统的<em class='search-result-highlight'>爬虫<\/em>程序从初始web页面的一个或多个url开始，并获取初始web页面的url。在抓取web页面的过程中，它不断地从当前页面中提取新的url并将其放入队列中，直到满足系统的某些停止条件\u2026\u2026。聚焦<em class='search-result-highlight'>爬虫<\/em>的工作流程比较复杂。需要根据一定的网页分析算法对与主题无关的链接进行过滤，保留有用的链接并将其放入正在等待的URL队列中。 然后，根据一定的搜索策略，从队列中选择要抓取的下一个页面URL，重复\u2026\u2026这个过程，直到达到系统的一定条件。此外，<em class='search-result-highlight'>爬虫<\/em>程序捕获的所有web页面将由系统存储、分析、过滤和索引，以供后续查询和检索。 所以一个完整的<em class='search-result-highlight'>爬虫<\/em>一般会包含如下三个模块： 网络请求模块 爬取流程控制模块 内容\u2026\u2026","user":{"id":15184619,"nickname":"Python学习者1","slug":"78c40b03ee4e","avatar_url":"http://upload.jianshu.io/users/upload_avatars/15184619/5d8ad8e2-49ce-42bd-a195-705da1fcc069.jpg"},"notebook":{"id":31846901,"name":"日记本"},"commentable":true,"public_comments_count":0,"likes_count":7,"views_count":165,"total_rewards_count":0,"first_shared_at":"2019-01-28T13:22:28.000Z"},{"id":18037458,"title":"学会运用<em class='search-result-highlight'>爬虫<\/em>框架 Scrapy (五)  \u2014\u2014 部署<em class='search-result-highlight'>爬虫<\/em>","slug":"1b84c26ddf06","content":"\u2026\u2026本文是 Scrapy <em class='search-result-highlight'>爬虫<\/em>系列的最后一篇文章。主要讲述如何将我们编写的<em class='search-result-highlight'>爬虫<\/em>程序部署到生产环境中。我们使用由 scrapy 官方提供的<em class='search-result-highlight'>爬虫<\/em>管理工具 scrapyd 来部署<em class='search-result-highlight'>爬虫<\/em>程序。 1 为什么使用\u2026\u2026 scrapyd? 一是它由 scrapy 官方提供的，二是我们使用它可以非常方便地运用 JSON API来部署<em class='search-result-highlight'>爬虫<\/em>、控制<em class='search-result-highlight'>爬虫<\/em>以及查看运行日志。 2 使用 scrapyd 2.1 原理 选择一台主机当做\u2026\u2026服务器，安装并启动 scrapyd 服务。再这之后，scrapyd 会以守护进程的方式存在系统中，监听<em class='search-result-highlight'>爬虫<\/em>地运行与请求，然后启动进程来执行<em class='search-result-highlight'>爬虫<\/em>程序。 2.2 安装 scrapyd 使用 pip 能比较\u2026\u2026","user":{"id":7931281,"nickname":"猴哥Yuri","slug":"f46becd1ed83","avatar_url":"http://upload.jianshu.io/users/upload_avatars/7931281/0631d979-385e-45fb-8525-2eb7c6c9824b.jpg"},"notebook":{"id":16590307,"name":"Python 爬虫"},"commentable":true,"public_comments_count":4,"likes_count":8,"views_count":880,"total_rewards_count":0,"first_shared_at":"2017-10-09T12:59:05.000Z"},{"id":13367882,"title":"学<em class='search-result-highlight'>爬虫<\/em>先学什么？写给小白的python<em class='search-result-highlight'>爬虫<\/em>入门方法论（第三期）","slug":"ceb6b6cea284","content":"\u2026\u2026编者注：这是笔者基于自身在入门python<em class='search-result-highlight'>爬虫<\/em>一些感悟，而写作的\u2014\u2014入门小参考或建议。本文没有过多讲述学习<em class='search-result-highlight'>爬虫<\/em>需要哪些库或代码，而是期望为初学者提供一些<em class='search-result-highlight'>爬虫<\/em>思维或方法论，从而快速入门。不过，每个人的\u2026\u2026基础不同，这仅是一家之言，希望大家能有所收获。 （1）我们并不缺少python<em class='search-result-highlight'>爬虫<\/em>的各类教程学<em class='search-result-highlight'>爬虫<\/em>先学什么？有人说是编程，对也不对。对的是<em class='search-result-highlight'>爬虫<\/em>也是以一定的编程语言为基础的，对于连编程都不是很熟悉的纯小\u2026\u2026白来说，建议你去从编程学起。不对，是因为对于已经有些编程基础的<em class='search-result-highlight'>爬虫<\/em>小白来说，学习到python<em class='search-result-highlight'>爬虫<\/em>的编程套路，你也不一定会真正了解<em class='search-result-highlight'>爬虫<\/em>，灵活运用。事实上，我们并不缺少python<em class='search-result-highlight'>爬虫<\/em>的各类教程，在网络\u2026\u2026","user":{"id":3605636,"nickname":"博观厚积","slug":"2f376f777ef1","avatar_url":"http://upload.jianshu.io/users/upload_avatars/3605636/1d6b0b3e9ce6.jpg"},"notebook":{"id":7470116,"name":"python爬虫"},"commentable":true,"public_comments_count":4,"likes_count":25,"views_count":1639,"total_rewards_count":1,"first_shared_at":"2017-06-11T15:38:22.000Z"},{"id":27825686,"title":"python3 分布式<em class='search-result-highlight'>爬虫<\/em>","slug":"ec3dfaec3c9b","content":"\u2026\u2026背景 部门（东方IC、图虫）业务驱动，需要搜集大量图片资源，做数据分析，以及正版图片维权。前期主要用node做<em class='search-result-highlight'>爬虫<\/em>(业务比较简单，对node比较熟悉)。随着业务需求的变化，大规模<em class='search-result-highlight'>爬虫<\/em>遇到各种问题\u2026\u2026。python<em class='search-result-highlight'>爬虫<\/em>具有先天优势，社区资源比较齐全，各种框架也完美支持。<em class='search-result-highlight'>爬虫<\/em>性能也得到极大提升。本次分享从基础知识入手，涉及python 的两大<em class='search-result-highlight'>爬虫<\/em>框架pyspider、scrapy，并基于scrapy\u2026\u2026、scrapy-redis 做了分布式<em class='search-result-highlight'>爬虫<\/em>的介绍（直接粘贴的ppt截图）会涉及 redis、mongodb等相关知识。 对于反防盗链（自动登录、自动注册... 以及常见策略）、代理、<em class='search-result-highlight'>爬虫<\/em>快照、对象资源入\u2026\u2026","user":{"id":11994763,"nickname":"字节跳动技术团队","slug":"b3f97f005731","avatar_url":"http://upload.jianshu.io/users/upload_avatars/11994763/707a4c4a-eb52-4b39-b891-2faceed38262.png"},"notebook":{"id":25119199,"name":"日记本"},"commentable":true,"public_comments_count":1,"likes_count":61,"views_count":1628,"total_rewards_count":0,"first_shared_at":"2018-05-10T06:28:08.000Z"},{"id":38569268,"title":"学<em class='search-result-highlight'>爬虫<\/em>必备32个项目，学会可以出师了！","slug":"13511d26584e","content":"\u2026\u2026今天为大家整理了32个Python<em class='search-result-highlight'>爬虫<\/em>项目 整理的原因是，<em class='search-result-highlight'>爬虫<\/em>入门简单快速，也非常适合新入门的小伙伴培养信心。所有链接指向GitHub，祝大家玩的愉快~O(∩_∩)O 学习Python中有不明白推荐\u2026\u2026<em class='search-result-highlight'>爬虫<\/em>。基于搜狗微信搜索的微信公众号<em class='search-result-highlight'>爬虫<\/em>接口，可以扩展成基于搜狗搜索的<em class='search-result-highlight'>爬虫<\/em>，返回结果是列表，每一项均是公众号具体信息字典。 DouBanSpider [2]- 豆瓣读书<em class='search-result-highlight'>爬虫<\/em>。可以爬下豆瓣读书标签下的所有\u2026\u2026，并加入随机延时来更好的模仿浏览器行为，避免<em class='search-result-highlight'>爬虫<\/em>被封。 zhihu_spider [3]- 知乎<em class='search-result-highlight'>爬虫<\/em>。此项目的功能是爬取知乎用户信息以及人际拓扑关系，<em class='search-result-highlight'>爬虫<\/em>框架使用scrapy，数据存储使用mongo\u2026\u2026","user":{"id":15344105,"nickname":"山禾家的猫","slug":"9262b0a0796c","avatar_url":"http://upload.jianshu.io/users/upload_avatars/15344105/4abb9149-f330-4367-9dd1-1393d591f724.jpg"},"notebook":{"id":32181612,"name":"日记本"},"commentable":true,"public_comments_count":0,"likes_count":1,"views_count":58,"total_rewards_count":0,"first_shared_at":"2018-12-19T07:05:16.000Z"},{"id":42818473,"title":"Python<em class='search-result-highlight'>爬虫<\/em>的作用与地位（附python3教程+<em class='search-result-highlight'>爬虫<\/em>技术路线图）","slug":"ca0d4a75f246","content":"\u2026\u2026小编说：网络<em class='search-result-highlight'>爬虫<\/em>是一种伴随着互联网诞生与演化的\u201c古老\u201d的网络技术，随着互联网进入大数据时代，<em class='search-result-highlight'>爬虫<\/em>技术迎来了一波新的振兴浪潮。 本文通过企业内部与互联网两个场景向大家讲书<em class='search-result-highlight'>爬虫<\/em>发挥了哪些重要作用。本文\u2026\u2026选自《虫术\u2014\u2014Python绝技》一书。 在大数据架构中，数据收集与数据存储占据了极为重要的地位，可以说是大数据的核心基础。而<em class='search-result-highlight'>爬虫<\/em>技术在这两大核心技术层次中占有了很大的比例。为何有此一说？我们不妨通过一个\u2026\u2026实际应用场景来看看<em class='search-result-highlight'>爬虫<\/em>到底发挥了哪些作用？ 在了解<em class='search-result-highlight'>爬虫<\/em>的作用之前，应该先了解其基本特性： 主动\u2014\u2014<em class='search-result-highlight'>爬虫<\/em>的重点在于\u201c爬取\u201d（Crawl），这是一种主动性的行为。换句话说，它是一个可以独立运行且能按照一定\u2026\u2026","user":{"id":7600417,"nickname":"梦想编程家小枫","slug":"5d73c449062e","avatar_url":"http://upload.jianshu.io/users/upload_avatars/7600417/8bc068e4-addf-4e1c-9801-3cec21f2b833.png"},"notebook":{"id":15849659,"name":"日记本"},"commentable":true,"public_comments_count":0,"likes_count":0,"views_count":5,"total_rewards_count":0,"first_shared_at":"2019-03-11T06:56:16.000Z"}]
     * related_users : []
     * more_related_users : null
     * related_collections : []
     * more_related_collections : null
     */

    public String q;
    public int page;
    public String type;
    public int total_count;
    public int per_page;
    public int total_pages;
    public String order_by;
    public Object more_related_users;
    public Object more_related_collections;
    public List<EntriesBean> entries;
    public List<?> related_users;
    public List<?> related_collections;

    public static class EntriesBean {
        /**
         * id : 28702113
         * title : 【Python实战】用Scrapyd把Scrapy<em class='search-result-highlight'>爬虫</em>一步一步部署到腾讯云上，有彩蛋
         * slug : af98a1b72b3e
         * content : ……接着之前的几篇文章说。我把<em class='search-result-highlight'>爬虫</em>已经写好了，而且在本地可以运行了。这个不是最终的目的啊。我们是要在服务器上运行<em class='search-result-highlight'>爬虫</em>。利用周末，同时腾讯送的7天云服务器体验也快到期了就在这里再来一篇手把手的将<em class='search-result-highlight'>爬虫</em>部署到……服务器上吧。绝对从0教学。一步一步的来，还有截图让你从『倔强青铜』杀到『最强王者』 为啥要写这篇文章，就是为了让你上『最强王者』！ Scrapy的文章，好多好多，但是99%的文章都是，写完<em class='search-result-highlight'>爬虫</em>就完事儿……了，至于后来怎么用？去哪里用？都没有交带。我这里就交代一种，可以把你的小虫子部署到服务器上！但是怎么部署，当你去百度查『scrapy<em class='search-result-highlight'>爬虫</em>部署』的时候，有几篇文章说，用Scrapyd，但是，他们都只是……
         * user : {"id":4417378,"nickname":"皮克啪的铲屎官","slug":"c2aa1d94244a","avatar_url":"http://upload.jianshu.io/users/upload_avatars/4417378/3160dc89-7fd3-457e-aa96-1616f9a22080"}
         * notebook : {"id":25265876,"name":"Python"}
         * commentable : true
         * public_comments_count : 1
         * likes_count : 13
         * views_count : 1055
         * total_rewards_count : 0
         * first_shared_at : 2018-05-28T04:59:26.000Z
         */

        public int id;
        public String title;
        public String slug;
        public String content;
        public UserBean user;
        public NotebookBean notebook;
        public boolean commentable;
        public int public_comments_count;
        public int likes_count;
        public int views_count;
        public int total_rewards_count;
        public String first_shared_at;

        public static class UserBean {
            /**
             * id : 4417378
             * nickname : 皮克啪的铲屎官
             * slug : c2aa1d94244a
             * avatar_url : http://upload.jianshu.io/users/upload_avatars/4417378/3160dc89-7fd3-457e-aa96-1616f9a22080
             */

            public int id;
            public String nickname;
            public String slug;
            public String avatar_url;
        }

        public static class NotebookBean {
            /**
             * id : 25265876
             * name : Python
             */

            public int id;
            public String name;
        }
    }
}
